import streamlit as st
import sys
import os
import time
import tempfile
import arxiv
import requests
import math
import re
import csv
import hashlib
from datetime import datetime
from io import StringIO, BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import bibtexparser
    from bibtexparser.bwriter import BibTexWriter
    from bibtexparser.bibdatabase import BibDatabase
    import zhipuai
    import langchain_community
    import fitz  # pymupdf
except ImportError as e:
    st.error(f"ğŸš‘ ç¯å¢ƒç¼ºå¤±åº“ -> {e.name}")
    st.info("è¯·æ‰§è¡Œå®‰è£…å‘½ä»¤ï¼špip install zhipuai langchain-community pymupdf arxiv requests streamlit-agraph faiss-cpu bibtexparser")
    st.stop()

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.chat_models import ChatZhipuAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from streamlit_agraph import agraph, Node, Edge, Config

# ================= 2. é¡µé¢é…ç½® =================
st.set_page_config(page_title="ç§‘ç ”æ–‡çŒ®åŠ©æ‰‹ | Connected Papers + AI ç²¾è¯»", layout="wide", page_icon="ğŸ“")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px;}
    .reportview-container { margin-top: -2em; }
    .abstract-box {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 8px;
        border-left: 5px solid #4CAF50;
        font-size: 0.95em;
        line-height: 1.6;
        margin-bottom: 10px;
    }
    .cite-badge {
        background-color: #ff4b4b;
        color: white;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.8em;
        font-weight: bold;
    }
    .high-impact {
        background-color: #fff3cd;
        padding: 2px 6px;
        border-radius: 4px;
        font-size: 0.75em;
        color: #856404;
        font-weight: bold;
    }
    .detail-panel {
        background-color: #ffffff;
        padding: 20px;
        border-radius: 10px;
        border: 1px solid #ddd;
        height: 600px;
        overflow-y: auto;
    }
    .timeline-note {
        font-size: 0.8em;
        color: #666;
        text-align: center;
        margin-top: 10px;
    }
</style>
""", unsafe_allow_html=True)
st.title("ğŸ“ ç§‘ç ”æ–‡çŒ®åŠ©æ‰‹ | Connected Papers + AI ç²¾è¯»")

# ================= 3. çŠ¶æ€åˆå§‹åŒ– =================
for key, default in [
    ("chat_history", []),
    ("db", None),
    ("loaded_files", []),
    ("all_chunks", []),
    ("suggested_query", ""),
    ("search_results", []),
    ("selected_scope", "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"),
    ("focus_paper_id", None),
    ("filtered_results", []),
    ("pending_ai_prompt", None),  # æ–°å¢ï¼šå¿«æ·æŒ‰é’®è§¦å‘åå¾…å¤„ç†çš„ prompt
]:
    if key not in st.session_state:
        st.session_state[key] = default

# ================= 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° =================

def get_pure_arxiv_id(url_or_id):
    """ä» URL æˆ–å­—ç¬¦ä¸²ä¸­ç²¾å‡†æå– ArXiv ID"""
    match = re.search(r'(\d{4}\.\d{4,5})', url_or_id)
    if match:
        return match.group(1)
    return url_or_id.split('/')[-1].split('v')[0]

def _hash_key(key):
    """å¯¹ API Key åšå•å‘å“ˆå¸Œï¼Œé¿å…æ˜æ–‡å­˜å…¥ç¼“å­˜é”®"""
    if not key:
        return ""
    return hashlib.sha256(key.encode()).hexdigest()[:16]

def fetch_citations_single(arxiv_id, ss_key=None):
    """è·å–å•ç¯‡è®ºæ–‡å¼•ç”¨æ•°"""
    try:
        clean_id = get_pure_arxiv_id(arxiv_id)
        api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields=citationCount"
        headers = {"x-api-key": ss_key} if ss_key else {}
        delay = 0.05 if ss_key else 1.0
        time.sleep(delay)
        response = requests.get(api_url, headers=headers, timeout=5)
        if response.status_code == 200:
            return response.json().get('citationCount', 0)
    except Exception:
        pass
    return 0

def fetch_citations_batch(arxiv_ids, ss_key=None):
    """
    å¹¶å‘è·å–å¤šç¯‡è®ºæ–‡å¼•ç”¨æ•°ï¼ˆä¿®å¤ï¼šä¸²è¡Œæ”¹å¹¶å‘ï¼‰
    æ—  Key é™åˆ¶å¹¶å‘æ•°ä¸º 2ï¼Œæœ‰ Key å¯æé«˜åˆ° 5
    """
    max_workers = 5 if ss_key else 2

    def _fetch(aid):
        return aid, fetch_citations_single(aid, ss_key)

    results = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(_fetch, aid): aid for aid in arxiv_ids}
        for future in as_completed(futures):
            aid, count = future.result()
            results[aid] = count
    return results

@st.cache_data(ttl=3600)
def fetch_graph_data(arxiv_id, ss_key_hash="", ss_key="", expand_depth=1):
    """
    è·å–å›¾è°±æ•°æ®
    - ss_key_hash ä½œä¸ºç¼“å­˜é”®ï¼ˆä¸å­˜æ˜æ–‡ï¼‰
    - expand_depth: 1=ç›´æ¥å¼•ç”¨/è¢«å¼•, 2=é€’å½’æ‹“å±•ä¸€å±‚
    """
    clean_id = get_pure_arxiv_id(arxiv_id)
    fields = (
        "paperId,title,year,citationCount,abstract,url,journal,"
        "references.paperId,references.title,references.citationCount,"
        "references.year,references.abstract,references.url,"
        "citations.paperId,citations.title,citations.citationCount,"
        "citations.year,citations.abstract,citations.url"
    )
    api_url = f"https://api.semanticscholar.org/graph/v1/paper/ArXiv:{clean_id}?fields={fields}"
    headers = {"x-api-key": ss_key} if ss_key else {}

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.get(api_url, headers=headers, timeout=12)
            if response.status_code == 200:
                data = response.json()

                # é€’å½’æ‹“å±•ï¼ˆé¢†åŸŸçº§è„‰ç»œï¼‰
                # ä¿®å¤ï¼šæ¬¡çº§å¼•ç”¨çš„ paperId æ˜¯ SS å†…éƒ¨ IDï¼Œä¸æ˜¯ ArXiv IDï¼Œ
                # éœ€è¦ç”¨ SS paper ID æ¥å£è€Œé ArXiv æ¥å£
                if expand_depth > 1:
                    for ref in data.get('references', [])[:5]:
                        ref_pid = ref.get('paperId')
                        if ref_pid:
                            sub_url = (
                                f"https://api.semanticscholar.org/graph/v1/paper/{ref_pid}"
                                f"?fields=references.paperId,references.title,references.citationCount,references.year"
                            )
                            try:
                                sub_resp = requests.get(sub_url, headers=headers, timeout=8)
                                if sub_resp.status_code == 200:
                                    ref['sub_references'] = sub_resp.json().get('references', [])[:3]
                            except Exception:
                                pass

                return data
            elif response.status_code == 429:
                time.sleep((attempt + 1) * 2)
                continue
            else:
                return None
        except Exception:
            if attempt == max_retries - 1:
                return None
    return None

def render_research_graph(data, min_citation=10, year_range=(2010, 2026)):
    """
    ç§‘ç ”çº§è„‰ç»œå›¾è°±æ¸²æŸ“
    ä¿®å¤ï¼šç§»é™¤ä¸æ”¯æŒçš„ Python lambdaï¼Œæ”¹ç”¨é™æ€ç‰©ç†å¼•æ“é…ç½®
    """
    if not data:
        return None, {}

    nodes, edges = [], []
    paper_details = {}
    min_year, max_year = year_range

    def get_color(rel_type):
        colors = {
            'seed': "#FF4B4B",
            'reference': "#2563eb",
            'citation': "#059669",
            'sub_reference': "#60a5fa"
        }
        return colors.get(rel_type, "#94a3b8")

    def get_node_size(citation_count):
        if citation_count < min_citation:
            return 10
        return max(15, 15 + (math.log(citation_count + 1) * 4))

    def is_high_impact(cites):
        return cites >= 50

    def year_in_range(year):
        if year == 'Unknown':
            return True
        try:
            y = int(year)
            return min_year <= y <= max_year
        except (ValueError, TypeError):
            return True

    # ç§å­èŠ‚ç‚¹
    seed_id = data.get('paperId', 'root')
    seed_title = data.get('title', 'Seed Paper') or 'Seed Paper'
    seed_cites = data.get('citationCount', 0) or 0
    seed_year = data.get('year', 'Unknown')

    if not year_in_range(seed_year):
        return None, {}

    paper_details[seed_id] = {
        "title": seed_title,
        "abstract": data.get('abstract') or "æ— æ‘˜è¦",
        "year": seed_year,
        "cites": seed_cites,
        "url": data.get('url') or f"https://www.semanticscholar.org/paper/{seed_id}",
        "journal": (data.get('journal') or {}).get('name') or "æœªçŸ¥æœŸåˆŠ",
        "is_high_impact": is_high_impact(seed_cites)
    }

    nodes.append(Node(
        id=seed_id,
        label=f"æ ¸å¿ƒ\n{seed_title[:12]}...",
        size=40 if is_high_impact(seed_cites) else 35,
        color=get_color('seed'),
        title=f"{seed_title}\nå¼•ç”¨æ•°: {seed_cites}\nå¹´ä»½: {seed_year}\n{'ğŸ”¥ é«˜å½±å“åŠ›' if is_high_impact(seed_cites) else ''}"
    ))

    seen_ids = {seed_id}

    def add_paper_node(p, rel_type):
        p_id = p.get('paperId')
        if not p_id or p_id in seen_ids:
            return False
        p_cites = p.get('citationCount') or 0
        p_year = p.get('year', 'Unknown')
        if p_cites < min_citation:
            return False
        if not year_in_range(p_year):
            return False

        seen_ids.add(p_id)
        p_title = p.get('title') or 'Unknown'

        paper_details[p_id] = {
            "title": p_title,
            "abstract": p.get('abstract') or "æš‚æ— è¯¦ç»†æ‘˜è¦",
            "year": p_year,
            "cites": p_cites,
            "url": p.get('url') or f"https://www.semanticscholar.org/paper/{p_id}",
            "journal": "æœªçŸ¥æœŸåˆŠ",
            "is_high_impact": is_high_impact(p_cites)
        }

        nodes.append(Node(
            id=p_id,
            label=f"{p_title[:12]}...",
            size=get_node_size(p_cites),
            color=get_color(rel_type),
            title=f"{p_title}\nå¼•ç”¨æ•°: {p_cites}\nå¹´ä»½: {p_year}\n{'ğŸ”¥ é«˜å½±å“åŠ›' if is_high_impact(p_cites) else ''}"
        ))
        return True

    # å‚è€ƒæ–‡çŒ®ï¼ˆè“è‰²ï¼šå‰äººå·¥ä½œï¼‰
    for p in data.get('references', [])[:20]:
        if add_paper_node(p, 'reference'):
            p_id = p['paperId']
            p_cites = p.get('citationCount') or 0
            edges.append(Edge(
                source=seed_id,
                target=p_id,
                color="#94a3b8",
                width=2 if is_high_impact(p_cites) else 1.5,
                label="å¼•ç”¨"
            ))

    # æ–½å¼•æ–‡çŒ®ï¼ˆç»¿è‰²ï¼šåç»­å‘å±•ï¼‰
    for p in data.get('citations', [])[:20]:
        if add_paper_node(p, 'citation'):
            p_id = p['paperId']
            p_cites = p.get('citationCount') or 0
            edges.append(Edge(
                source=p_id,
                target=seed_id,
                color="#d1d5db",
                width=2 if is_high_impact(p_cites) else 1,
                label="è¢«å¼•"
            ))

    # ä¿®å¤ï¼šç§»é™¤ Python lambdaï¼Œä½¿ç”¨é™æ€é…ç½®
    config = Config(
        width="100%",
        height=700,
        directed=True,
        physics=True,
        nodeHighlightBehavior=True,
        highlightColor="#F7D154",
        collapsible=False,
        staticGraph=False,
        d3={
            'alphaTarget': 0.08,
            'gravity': -300,
            'linkLength': 180,
            'linkStrength': 0.15,
        }
    )

    clicked_id = agraph(nodes=nodes, edges=edges, config=config)
    return clicked_id, paper_details

def fix_latex_errors(text):
    if not text:
        return text
    text = text.replace(r"\(", "$").replace(r"\)", "$")
    text = text.replace(r"\[", "$$").replace(r"\]", "$$")
    return text

def require_api_key(key, label="æ™ºè°± API Key"):
    """å®ˆå«å‡½æ•°ï¼šæœªå¡« Key æ—¶æ˜¾ç¤ºè­¦å‘Šå¹¶åœæ­¢"""
    if not key:
        st.warning(f"âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ å¡«å…¥ {label}")
        st.stop()

def rebuild_index_from_chunks(api_key):
    if not st.session_state.all_chunks:
        st.session_state.db = None
        return
    embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)
    st.session_state.db = FAISS.from_documents(st.session_state.all_chunks, embeddings)

def process_and_add_to_db(file_path, file_name, api_key, cleanup=True):
    """
    è§£æ PDF å¹¶åŠ å…¥å‘é‡åº“
    ä¿®å¤ï¼šä½¿ç”¨ try/finally ä¿è¯ä¸´æ—¶æ–‡ä»¶æ¸…ç†
    ä¿®å¤ï¼šé˜²æ­¢é‡å¤åŠ è½½åŒä¸€æ–‡ä»¶
    """
    try:
        if file_name in st.session_state.loaded_files:
            st.warning(f"ã€Š{file_name}ã€‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤å¯¼å…¥ã€‚")
            return

        loader = PyPDFLoader(file_path)
        docs = loader.load()
        for doc in docs:
            doc.metadata['source_paper'] = file_name

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        chunks = splitter.split_documents(docs)
        valid_chunks = [c for c in chunks if len(c.page_content.strip()) > 20]

        st.session_state.all_chunks.extend(valid_chunks)
        embeddings = ZhipuAIEmbeddings(model="embedding-2", api_key=api_key)

        batch_size = 10
        total = len(valid_chunks)
        if st.session_state.db is None:
            st.session_state.db = FAISS.from_documents(valid_chunks[:batch_size], embeddings)
            for i in range(batch_size, total, batch_size):
                st.session_state.db.add_documents(valid_chunks[i: i + batch_size])
                time.sleep(0.1)
        else:
            for i in range(0, total, batch_size):
                st.session_state.db.add_documents(valid_chunks[i: i + batch_size])
                time.sleep(0.1)

        st.session_state.loaded_files.append(file_name)
        st.session_state.chat_history.append({
            "role": "system_notice",
            "content": f"ğŸ“š **ç³»ç»Ÿé€šçŸ¥**ï¼šå·²åŠ è½½ã€Š{file_name}ã€‹ã€‚"
        })
    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {e}")
    finally:
        # ä¿®å¤ï¼šä¿è¯ä¸´æ—¶æ–‡ä»¶ä¸€å®šè¢«åˆ é™¤
        if cleanup and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception:
                pass

# ================= 5. ç§‘ç ”çº§å¯¼å‡ºåŠŸèƒ½ =================
def export_papers_to_csv(paper_details):
    output = StringIO()
    writer = csv.writer(output)
    writer.writerow(['æ ‡é¢˜', 'å¹´ä»½', 'å¼•ç”¨æ•°', 'æœŸåˆŠ', 'é“¾æ¥', 'æ‘˜è¦', 'æ˜¯å¦é«˜å½±å“åŠ›'])
    for pid, info in paper_details.items():
        abstract = info['abstract']
        writer.writerow([
            info['title'],
            info['year'],
            info['cites'],
            info['journal'],
            info['url'],
            abstract[:200] + "..." if len(abstract) > 200 else abstract,
            "æ˜¯" if info['is_high_impact'] else "å¦"
        ])
    output.seek(0)
    return output

def export_papers_to_bibtex(paper_details):
    """
    å¯¼å‡º BibTeX
    ä¿®å¤ï¼šè¡¥å…¨ author/journal å­—æ®µï¼Œä½¿ç”¨è§„èŒƒ ID æ ¼å¼
    """
    db = BibDatabase()
    entries = []
    for idx, (pid, info) in enumerate(paper_details.items()):
        title = info['title']
        year = str(info['year']) if info['year'] != 'Unknown' else 'n.d.'
        # è§„èŒƒ IDï¼šå–æ ‡é¢˜é¦–è¯ + å¹´ä»½
        first_word = re.sub(r'[^a-zA-Z]', '', title.split()[0]) if title else f"paper{idx}"
        entry_id = f"{first_word.lower()}{year}"

        entry = {
            'ID': entry_id,
            'ENTRYTYPE': 'article',
            'title': title,
            'year': year,
            'journal': info.get('journal', 'Unknown'),
            'url': info['url'],
            'note': f"Citation count: {info['cites']}",
            'abstract': info['abstract'][:300] if info['abstract'] != "æš‚æ— è¯¦ç»†æ‘˜è¦" else '',
        }
        entries.append(entry)

    db.entries = entries
    writer = BibTexWriter()
    return writer.write(db)

# ================= 6. AI ç§‘ç ”ç»¼è¿° =================
@st.cache_resource
def get_llm(api_key, model="glm-4"):
    """ç¼“å­˜ LLM å®ä¾‹ï¼Œé¿å…æ¯æ¬¡å¯¹è¯é‡æ–°å®ä¾‹åŒ–"""
    return ChatZhipuAI(model=model, api_key=api_key, temperature=0.1)

def generate_research_summary(paper_details, api_key):
    llm = get_llm(api_key)
    high_impact_papers = [i for i in paper_details.values() if i['is_high_impact']]
    normal_papers = [i for i in paper_details.values() if not i['is_high_impact']]

    summary_context = "### é«˜å½±å“åŠ›æ ¸å¿ƒè®ºæ–‡ï¼ˆå¼•ç”¨â‰¥50ï¼‰\n"
    for info in high_impact_papers[:5]:
        summary_context += f"\n**{info['title']}** ({info['year']}, å¼•ç”¨{info['cites']})\n"
        summary_context += f"æ‘˜è¦æ ¸å¿ƒï¼š{info['abstract'][:300]}...\n"

    summary_context += "\n### é¢†åŸŸåç»­å‘å±•è®ºæ–‡\n"
    for info in normal_papers[:5]:
        summary_context += f"\n**{info['title']}** ({info['year']}, å¼•ç”¨{info['cites']})\n"

    prompt = f"""
è¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç§‘ç ”çº§é¢†åŸŸç»¼è¿°ï¼ˆä¸­æ–‡ï¼‰ï¼Œè¦æ±‚ï¼š
1. å…ˆæ€»ç»“è¯¥é¢†åŸŸçš„æ ¸å¿ƒç ”ç©¶é—®é¢˜å’Œå‘å±•è„‰ç»œï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
2. åˆ—å‡º3-5ç¯‡å…³é”®å¥ åŸºè®ºæ–‡åŠå…¶è´¡çŒ®
3. åˆ†æå½“å‰é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹å’Œä¸è¶³
4. ç»™å‡ºæœªæ¥ç ”ç©¶æ–¹å‘å»ºè®®

æ–‡çŒ®ä¿¡æ¯ï¼š
{summary_context}
"""
    response = llm.invoke(prompt)
    return fix_latex_errors(response.content)

def run_ai_chat(prompt, api_key, reading_mode, selected_scope):
    """
    æ‰§è¡Œ AI é—®ç­”é€»è¾‘ï¼ˆæŠ½ç¦»ä¸ºç‹¬ç«‹å‡½æ•°ï¼Œä¾›å¿«æ·æŒ‰é’®å’Œæ‰‹åŠ¨è¾“å…¥å…±ç”¨ï¼‰
    ä¿®å¤ï¼šFAISS filter æ”¹ä¸ºæ£€ç´¢åæ‰‹åŠ¨è¿‡æ»¤
    """
    require_api_key(api_key)
    if not st.session_state.db:
        st.warning("ğŸ§  è¯·å…ˆä¸Šä¼ /åŠ è½½è®ºæ–‡åˆ°ç²¾è¯»åº“")
        return None

    search_k = 20 if "ç§‘ç ”ç²¾è¯»" in reading_mode else 10

    try:
        docs = st.session_state.db.max_marginal_relevance_search(
            prompt,
            k=search_k * 2,  # å¤šå–ä¸€äº›å†æ‰‹åŠ¨è¿‡æ»¤
            fetch_k=60,
            lambda_mult=0.7,
        )

        # ä¿®å¤ï¼šæ‰‹åŠ¨è¿‡æ»¤ï¼Œæ›¿ä»£ä¸ç¨³å®šçš„ filter å‚æ•°
        if selected_scope != "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡":
            docs = [d for d in docs if d.metadata.get('source_paper') == selected_scope]

        docs = docs[:search_k]

        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·è°ƒæ•´é—®é¢˜æˆ–åŠ è½½æ›´å¤šè®ºæ–‡ã€‚"

        context = "\n\n".join([
            f"ğŸ“„ã€{d.metadata.get('source_paper', 'æœªçŸ¥è®ºæ–‡')} ç¬¬{d.metadata.get('page', 0) + 1}é¡µã€‘:\n{d.page_content}"
            for d in docs
        ])

        sys_prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„ç§‘ç ”åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹å›ç­”é—®é¢˜ï¼Œè¦æ±‚ï¼š
1. å›ç­”å¿…é¡»åŸºäºè®ºæ–‡åŸæ–‡ï¼Œæ ‡æ³¨æ¥æºï¼ˆè®ºæ–‡å+é¡µç ï¼‰
2. ç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹è¯´æ˜ï¼Œå…¬å¼ç”¨ $ åŒ…è£¹
3. å®¢è§‚ä¸­ç«‹ï¼Œä¸ç¼–é€ ä¿¡æ¯
4. ä¼˜å…ˆä½¿ç”¨å­¦æœ¯æœ¯è¯­ï¼Œä¿æŒä¸“ä¸šæ€§

è®ºæ–‡å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{prompt}
"""
        llm = get_llm(api_key)
        response = llm.invoke(sys_prompt)
        return fix_latex_errors(response.content)

    except Exception as e:
        return f"ç”Ÿæˆå‡ºé”™: {e}"

# ================= 7. ä¾§è¾¹æ  =================
with st.sidebar:
    st.header("ğŸ›ï¸ ç§‘ç ”æ§åˆ¶å°")
    user_api_key = st.text_input("æ™ºè°± API Key", type="password")
    ss_api_key = st.text_input(
        "Semantic Scholar API Key",
        type="password",
        help="å¡«å…¥åå¤§å¹…æå‡é€Ÿç‡ï¼Œæ— åˆ™ä½¿ç”¨åŒ¿åæ¨¡å¼"
    )
    if ss_api_key:
        st.success("ğŸš€ ç§‘ç ”é«˜é€Ÿæ¨¡å¼å·²æ¿€æ´»")
    else:
        st.caption("ğŸ¢ åŒ¿åé™é€Ÿæ¨¡å¼ï¼ˆå¹¶å‘æ•°å—é™ï¼‰")

    st.markdown("---")

    st.subheader("âš¡ é«˜è´¨é‡è®ºæ–‡ç­›é€‰")
    min_cite_filter = st.slider("æœ€ä½å¼•ç”¨æ•°ï¼ˆè¿‡æ»¤ä½è´¨é‡è®ºæ–‡ï¼‰", 0, 100, 5, step=5)
    min_year_filter = st.slider("å‘è¡¨èµ·å§‹å¹´ä»½", 2000, 2026, 2015)
    expand_depth = st.radio("å›¾è°±æ‹“å±•æ·±åº¦", ["åŸºç¡€ï¼ˆç›´æ¥å¼•ç”¨ï¼‰", "é¢†åŸŸçº§ï¼ˆé€’å½’æ‹“å±•ï¼‰"], index=0)
    expand_depth_num = 1 if expand_depth == "åŸºç¡€ï¼ˆç›´æ¥å¼•ç”¨ï¼‰" else 2

    st.markdown("---")

    if st.session_state.loaded_files:
        st.subheader("ğŸ—‚ï¸ æ–‡çŒ®ç®¡ç†")
        for file in list(st.session_state.loaded_files):
            col_f1, col_f2 = st.columns([4, 1])
            with col_f1:
                st.text(f"ğŸ“„ {file[:18]}..." if len(file) > 20 else f"ğŸ“„ {file}")
            with col_f2:
                if st.button("ğŸ—‘ï¸", key=f"del_{file}"):
                    st.session_state.loaded_files.remove(file)
                    st.session_state.all_chunks = [
                        c for c in st.session_state.all_chunks
                        if c.metadata.get('source_paper') != file
                    ]
                    if user_api_key:
                        rebuild_index_from_chunks(user_api_key)
                    st.rerun()

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå…¨éƒ¨", type="primary"):
            st.session_state.db = None
            st.session_state.loaded_files = []
            st.session_state.all_chunks = []
            st.session_state.chat_history = []
            st.rerun()

    st.subheader("ğŸ“– ç ”è¯»æ¨¡å¼")
    reading_mode = st.radio("é€‰æ‹©æ¨¡å¼:", ["ğŸŸ¢ å¿«é€Ÿé—®ç­”", "ğŸ“– ç§‘ç ”ç²¾è¯»ï¼ˆç»“æ„åŒ–è§£æï¼‰"], index=1)

    if st.session_state.loaded_files:
        st.markdown("---")
        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            if st.button("ğŸ“Š ç”Ÿæˆè®ºæ–‡å¯¹æ¯”è¡¨"):
                require_api_key(user_api_key)
                if st.session_state.db:
                    with st.spinner("åˆ†æè®ºæ–‡ç‰¹å¾..."):
                        llm = get_llm(user_api_key)
                        aggregated_context = ""
                        for filename in st.session_state.loaded_files:
                            sub_docs = st.session_state.db.similarity_search(
                                "ç ”ç©¶ç›®æ ‡ åˆ›æ–°ç‚¹ æ–¹æ³• æ•°æ®é›† ç»“è®º å±€é™æ€§", k=3
                            )
                            sub_docs = [d for d in sub_docs if d.metadata.get('source_paper') == filename]
                            if sub_docs:
                                aggregated_context += f"\n=== {filename} ===\n" + "\n".join(
                                    [d.page_content for d in sub_docs]) + "\n"

                        prompt = f"""
è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆç§‘ç ”çº§å¯¹æ¯”è¡¨æ ¼ï¼ˆMarkdownæ ¼å¼ï¼‰ï¼Œåˆ—è¦æ±‚ï¼š
è®ºæ–‡å | ç ”ç©¶ç›®æ ‡ | æ ¸å¿ƒæ–¹æ³• | åˆ›æ–°ç‚¹ | æ•°æ®é›† | ä¸»è¦ç»“è®º | å±€é™æ€§

è®ºæ–‡å†…å®¹ï¼š
{aggregated_context}
"""
                        res = llm.invoke(prompt)
                        st.session_state.chat_history.append({"role": "assistant", "content": res.content})
                        st.rerun()

        with col_btn2:
            if st.button("ğŸ“ ç”Ÿæˆç»“æ„åŒ–ç»¼è¿°"):
                require_api_key(user_api_key)
                if st.session_state.db:
                    with st.spinner("ç”Ÿæˆé¢†åŸŸç»¼è¿°..."):
                        llm = get_llm(user_api_key)
                        aggregated_context = ""
                        for filename in st.session_state.loaded_files:
                            sub_docs = st.session_state.db.similarity_search(
                                "Abstract introduction conclusion contribution", k=5
                            )
                            sub_docs = [d for d in sub_docs if d.metadata.get('source_paper') == filename]
                            if sub_docs:
                                aggregated_context += f"\n=== {filename} ===\n" + "\n".join(
                                    [d.page_content for d in sub_docs]) + "\n"

                        prompt = f"""
è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–ç§‘ç ”ç»¼è¿°ï¼ˆä¸­æ–‡ï¼‰ï¼ŒåŒ…å«ï¼š
1. é¢†åŸŸèƒŒæ™¯ä¸ç ”ç©¶é—®é¢˜
2. æ ¸å¿ƒæ–¹æ³•ä¸æŠ€æœ¯è·¯çº¿
3. ä¸»è¦å‘ç°ä¸ç»“è®º
4. ç ”ç©¶ä¸è¶³ä¸æœªæ¥æ–¹å‘

è®ºæ–‡å†…å®¹ï¼š
{aggregated_context}
"""
                        res = llm.invoke(prompt)
                        st.session_state.chat_history.append({"role": "assistant", "content": res.content})
                        st.rerun()

        scope_options = ["ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡"] + st.session_state.loaded_files
        st.session_state.selected_scope = st.selectbox("ğŸ‘ï¸ ä¸“æ³¨èŒƒå›´", scope_options)

    st.markdown("---")
    st.subheader("ğŸ“¥ ä¸Šä¼ è®ºæ–‡")
    uploaded_file = st.file_uploader("æ‹–å…¥ PDF è®ºæ–‡", type="pdf")
    if uploaded_file and user_api_key and st.button("ç¡®è®¤åŠ è½½"):
        require_api_key(user_api_key)
        with st.spinner("è§£æPDFå¹¶æ„å»ºå‘é‡åº“..."):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                path = tmp.name
            # cleanup=True è®©å‡½æ•°å†…éƒ¨ finally è´Ÿè´£åˆ é™¤
            process_and_add_to_db(path, uploaded_file.name, user_api_key, cleanup=True)
            st.rerun()

# ================= 8. ä¸»ç•Œé¢ =================
tab_search, tab_chat = st.tabs(["ğŸ” æ–‡çŒ®è„‰ç»œï¼ˆConnected Papersï¼‰", "ğŸ’¬ ç§‘ç ”ç²¾è¯»"])

with tab_search:
    st.subheader("ğŸŒ å­¦æœ¯å¤§æ•°æ®æ£€ç´¢ï¼ˆç§‘ç ”çº§ï¼‰")
    col_q, col_sort, col_n = st.columns([3, 1.5, 1])
    with col_q:
        search_query = st.text_input(
            "å…³é”®è¯ï¼ˆæ”¯æŒ ti:æ ‡é¢˜ abs:æ‘˜è¦ è¯­æ³•ï¼‰",
            value=st.session_state.suggested_query,
            placeholder="ä¾‹å¦‚: ti:education AND abs:robot"
        )
    with col_sort:
        sort_mode = st.selectbox("æ’åºè§„åˆ™", ["ğŸ”¥ ç§‘ç ”å½±å“åŠ›ï¼ˆå¼•ç”¨+ç›¸å…³ï¼‰", "ğŸ“… æ—¶é—´ç”±æ–°åˆ°æ—§", "ğŸ“ˆ å¼•ç”¨é‡ç”±é«˜åˆ°ä½"])
    with col_n:
        max_results = st.number_input("è·å–æ•°é‡", min_value=5, max_value=50, value=15)

    if st.button("ğŸš€ å¼€å§‹æ£€ç´¢ï¼ˆè‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡è®ºæ–‡ï¼‰") and search_query:
        with st.spinner("æ£€ç´¢å¹¶åŒæ­¥ç§‘ç ”æ•°æ®..."):
            try:
                arxiv_sort = arxiv.SortCriterion.Relevance
                if "æ—¶é—´" in sort_mode:
                    arxiv_sort = arxiv.SortCriterion.SubmittedDate

                refined_query = search_query
                if " " in search_query and "AND" not in search_query and '"' not in search_query:
                    refined_query = " AND ".join([f'(ti:{w} OR abs:{w})' for w in search_query.split()])

                search = arxiv.Search(query=refined_query, max_results=max_results, sort_by=arxiv_sort)
                raw_results = list(search.results())

                # ä¿®å¤ï¼šå¹¶å‘è·å–å¼•ç”¨æ•°ï¼Œæ›¿ä»£ä¸²è¡Œ
                arxiv_ids = [r.entry_id for r in raw_results]
                progress_bar = st.progress(0, text="æ­£åœ¨å¹¶å‘è·å–å¼•ç”¨æ•°...")
                cite_map = fetch_citations_batch(arxiv_ids, ss_key=ss_api_key)
                progress_bar.progress(1.0, text="å®Œæˆï¼")

                results_with_cite = []
                for res in raw_results:
                    cites = cite_map.get(res.entry_id, 0)
                    if cites >= min_cite_filter:
                        results_with_cite.append({'obj': res, 'citations': cites})

                # ä¿®å¤ï¼šé™¤é›¶ä¿æŠ¤
                current_year = datetime.now().year
                if "ç§‘ç ”å½±å“åŠ›" in sort_mode:
                    results_with_cite.sort(
                        key=lambda x: (
                            x['citations'] * 0.7 +
                            (1 / max(1, current_year - x['obj'].published.year + 1)) * 0.3
                        ),
                        reverse=True
                    )
                elif "å¼•ç”¨é‡" in sort_mode:
                    results_with_cite.sort(key=lambda x: x['citations'], reverse=True)

                st.session_state.search_results = results_with_cite
                st.success(f"âœ… å®Œæˆï¼ç­›é€‰åå‰©ä½™ {len(results_with_cite)} ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼ˆå¼•ç”¨â‰¥{min_cite_filter}ï¼‰ã€‚")
            except Exception as e:
                st.error(f"æ£€ç´¢å¤±è´¥: {e}")

    # å›¾è°±å±•ç¤º
    if st.session_state.search_results:
        if st.session_state.focus_paper_id:
            st.markdown("---")
            st.subheader("ğŸ“Š ç§‘ç ”é¢†åŸŸè„‰ç»œå›¾è°±ï¼ˆConnected Papers é£æ ¼ï¼‰")
            st.markdown(
                '<div class="timeline-note">ğŸŸ¥ æ ¸å¿ƒè®ºæ–‡ | ğŸ”µ å‚è€ƒæ–‡çŒ®ï¼ˆå‰äººå·¥ä½œï¼‰ | ğŸŸ¢ æ–½å¼•æ–‡çŒ®ï¼ˆåç»­å‘å±•ï¼‰'
                ' | èŠ‚ç‚¹è¶Šå¤§=å¼•ç”¨è¶Šé«˜</div>',
                unsafe_allow_html=True
            )

            with st.spinner("æ„å»ºé¢†åŸŸè„‰ç»œå›¾è°±..."):
                # ä¿®å¤ï¼šä¼ å…¥å“ˆå¸Œåçš„ key ä½œä¸ºç¼“å­˜é”®ï¼ŒåŸå§‹ key ä½œä¸ºæ™®é€šå‚æ•°
                ss_key_hash = _hash_key(ss_api_key)
                g_data = fetch_graph_data(
                    st.session_state.focus_paper_id,
                    ss_key_hash=ss_key_hash,
                    ss_key=ss_api_key,
                    expand_depth=expand_depth_num
                )

            if not g_data:
                st.warning("âš ï¸ æš‚æ—¶æ— æ³•è·å–å›¾è°±æ•°æ®ã€‚è¯·æ£€æŸ¥ API Key æˆ–ç¨åå†è¯•ã€‚")
            else:
                col_graph, col_info = st.columns([2.5, 1])
                with col_graph:
                    clicked_node_id, all_details = render_research_graph(
                        g_data,
                        min_citation=min_cite_filter,
                        year_range=(min_year_filter, 2026)
                    )

                with col_info:
                    col_export1, col_export2 = st.columns(2)
                    with col_export1:
                        csv_data = export_papers_to_csv(all_details)
                        st.download_button(
                            label="ğŸ“¥ å¯¼å‡ºCSV",
                            data=csv_data,
                            file_name=f"research_papers_{datetime.now().strftime('%Y%m%d')}.csv",
                            mime="text/csv"
                        )
                    with col_export2:
                        bibtex_data = export_papers_to_bibtex(all_details)
                        st.download_button(
                            label="ğŸ“¥ å¯¼å‡ºBibTeX",
                            data=bibtex_data,
                            file_name=f"research_papers_{datetime.now().strftime('%Y%m%d')}.bib",
                            mime="text/plain"
                        )

                    if user_api_key and st.button("ğŸ“ ç”Ÿæˆé¢†åŸŸç»¼è¿°", use_container_width=True):
                        with st.spinner("AI åˆ†æé¢†åŸŸè„‰ç»œ..."):
                            summary = generate_research_summary(all_details, user_api_key)
                            st.session_state.chat_history.append({
                                "role": "assistant",
                                "content": f"## é¢†åŸŸç§‘ç ”ç»¼è¿°\n{summary}"
                            })
                            st.rerun()

                    if clicked_node_id and clicked_node_id in all_details:
                        info = all_details[clicked_node_id]
                        st.markdown("### ğŸ“‘ è®ºæ–‡è¯¦æƒ…")
                        st.markdown(f"**{info['title']}**")
                        if info['is_high_impact']:
                            st.markdown(
                                '<span class="high-impact">ğŸ”¥ é«˜å½±å“åŠ›è®ºæ–‡ï¼ˆå¼•ç”¨â‰¥50ï¼‰</span>',
                                unsafe_allow_html=True
                            )
                        c1, c2, c3 = st.columns(3)
                        c1.metric("ğŸ“… å¹´ä»½", info['year'])
                        c2.metric("ğŸ”¥ å¼•ç”¨", info['cites'])
                        journal_display = info['journal']
                        c3.metric("ğŸ“– æœŸåˆŠ", journal_display[:8] + "..." if len(journal_display) > 8 else journal_display)

                        st.markdown("---")
                        st.markdown(
                            f"**æ‘˜è¦**: \n\n <div style='font-size:0.85em; color:#444; height:200px;"
                            f" overflow-y:auto;'>{info['abstract']}</div>",
                            unsafe_allow_html=True
                        )
                        st.markdown("---")
                        st.link_button("ğŸŒ æŸ¥çœ‹åŸæ–‡", info['url'], use_container_width=True)
                    else:
                        st.info("""
ğŸ¯ **ç§‘ç ”å›¾è°±ä½¿ç”¨æŒ‡å—**
- ğŸ–±ï¸ ç‚¹å‡»èŠ‚ç‚¹ï¼šæŸ¥çœ‹è®ºæ–‡è¯¦æƒ…
- ğŸ”„ æ»šè½®ç¼©æ”¾ï¼šè°ƒæ•´å›¾è°±å¤§å°
- ğŸ“¥ å¯¼å‡ºï¼šç›´æ¥ç”¨äºè®ºæ–‡å¼•ç”¨/æ•´ç†
                        """)

        # è®ºæ–‡åˆ—è¡¨
        st.markdown("---")
        st.subheader(f"ğŸ“œ é«˜è´¨é‡è®ºæ–‡åˆ—è¡¨ï¼ˆå…± {len(st.session_state.search_results)} ç¯‡ï¼‰")
        for i, item in enumerate(st.session_state.search_results):
            res = item['obj']
            cites = item['citations']
            is_high = cites >= 50

            expander_title = f"#{i + 1} {'ğŸ”¥' if is_high else 'ğŸ“„'} {res.title} ({res.published.year}) | å¼•ç”¨: {cites}"
            with st.expander(expander_title):
                st.markdown(
                    f"**ğŸ‘¨â€ğŸ« ä½œè€…**: {', '.join([a.name for a in res.authors])} | "
                    f"**ğŸ“… å‘è¡¨**: {res.published.strftime('%Y-%m-%d')}"
                )
                if is_high:
                    st.markdown('<span class="high-impact">ğŸ”¥ é«˜å½±å“åŠ›è®ºæ–‡ï¼ˆä¼˜å…ˆç²¾è¯»ï¼‰</span>', unsafe_allow_html=True)
                st.markdown(
                    f'<div class="abstract-box"><b>ğŸ“ æ‘˜è¦ï¼š</b><br>{res.summary.replace(chr(10), " ")}</div>',
                    unsafe_allow_html=True
                )

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown(f"[ğŸ”— ArXiv åŸæ–‡]({res.entry_id})")
                with col2:
                    if st.button(f"â¬‡ï¸ åŠ å…¥ç²¾è¯»", key=f"dl_search_{i}"):
                        require_api_key(user_api_key)
                        with st.spinner("ä¸‹è½½å¹¶è§£æè®ºæ–‡..."):
                            tmp_path = None
                            try:
                                tmp_path = res.download_pdf(dirpath=tempfile.gettempdir())
                                process_and_add_to_db(tmp_path, res.title, user_api_key, cleanup=False)
                                st.success("âœ… å·²åŠ å…¥ç²¾è¯»åº“ï¼")
                            except Exception as e:
                                st.error(f"å¤±è´¥: {e}")
                            finally:
                                if tmp_path and os.path.exists(tmp_path):
                                    try:
                                        os.remove(tmp_path)
                                    except Exception:
                                        pass
                with col3:
                    if st.button(f"ğŸ•¸ï¸ æŸ¥çœ‹è„‰ç»œ", key=f"btn_graph_{i}"):
                        st.session_state.focus_paper_id = res.entry_id
                        st.rerun()

# ================= 9. ç§‘ç ”ç²¾è¯» Tab =================
with tab_chat:
    st.subheader("ğŸ’¬ ç§‘ç ”ç²¾è¯»ç©ºé—´ï¼ˆAI ç»“æ„åŒ–è§£æï¼‰")

    if st.session_state.loaded_files:
        st.caption(f"ğŸ“š æ¨¡å¼ï¼š{reading_mode} | èŒƒå›´ï¼š{st.session_state.selected_scope}")

        # ä¿®å¤ï¼šå¿«æ·æŒ‰é’®å°† prompt å­˜å…¥ pendingï¼Œæ¸²æŸ“å®ŒèŠå¤©è®°å½•åç»Ÿä¸€æ‰§è¡Œ AI è°ƒç”¨
        st.markdown("### âš¡ å¿«æ·ç§‘ç ”æé—®")
        col_quick1, col_quick2, col_quick3 = st.columns(3)
        with col_quick1:
            if st.button("ğŸ“Œ æå–åˆ›æ–°ç‚¹+æ–¹æ³•+ç»“è®º"):
                st.session_state.pending_ai_prompt = "è¯·æå–æ¯ç¯‡è®ºæ–‡çš„åˆ›æ–°ç‚¹ã€æ ¸å¿ƒæ–¹æ³•ã€ä¸»è¦ç»“è®ºï¼Œç”¨Markdownè¡¨æ ¼å±•ç¤º"
        with col_quick2:
            if st.button("ğŸ“Š å¯¹æ¯”è®ºæ–‡å·®å¼‚"):
                st.session_state.pending_ai_prompt = "å¯¹æ¯”è¿™äº›è®ºæ–‡çš„ç ”ç©¶æ–¹æ³•ã€åˆ›æ–°ç‚¹ã€å±€é™æ€§ï¼Œåˆ†æå„è‡ªçš„ä¼˜åŠ¿å’Œä¸è¶³"
        with col_quick3:
            if st.button("ğŸ“ ç”Ÿæˆç ”ç©¶æ€è·¯"):
                st.session_state.pending_ai_prompt = "åŸºäºè¿™äº›è®ºæ–‡ï¼Œç»™å‡ºè¯¥é¢†åŸŸçš„ç ”ç©¶æ€è·¯å’Œæœªæ¥æ–¹å‘å»ºè®®"

    # æ¸²æŸ“å†å²è®°å½•
    for msg in st.session_state.chat_history:
        if msg["role"] == "system_notice":
            st.info(msg["content"])
        else:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # ä¿®å¤ï¼šå¤„ç†å¿«æ·æŒ‰é’®è§¦å‘çš„ pending promptï¼Œæ‰§è¡ŒçœŸå® AI è°ƒç”¨
    if st.session_state.pending_ai_prompt:
        prompt = st.session_state.pending_ai_prompt
        st.session_state.pending_ai_prompt = None

        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AI æ­£åœ¨åˆ†æ..."):
                result = run_ai_chat(
                    prompt, user_api_key, reading_mode,
                    st.session_state.get("selected_scope", "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡")
                )
            if result:
                st.markdown(result)
                st.session_state.chat_history.append({"role": "assistant", "content": result})

    # æ‰‹åŠ¨è¾“å…¥
    if prompt := st.chat_input("è¾“å…¥ç§‘ç ”é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šè¿™ç¯‡è®ºæ–‡ç”¨äº†ä»€ä¹ˆæ•°æ®é›†ï¼Ÿå’ŒXXè®ºæ–‡çš„åˆ›æ–°ç‚¹å¯¹æ¯”ï¼Ÿï¼‰"):
        if not st.session_state.db:
            st.warning("ğŸ§  è¯·å…ˆä¸Šä¼ /åŠ è½½è®ºæ–‡åˆ°ç²¾è¯»åº“")
        else:
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.write(prompt)

            with st.chat_message("assistant"):
                with st.spinner("AI æ­£åœ¨åˆ†æ..."):
                    result = run_ai_chat(
                        prompt, user_api_key, reading_mode,
                        st.session_state.get("selected_scope", "ğŸŒ å¯¹æ¯”æ‰€æœ‰è®ºæ–‡")
                    )
                if result:
                    st.markdown(result)
                    st.session_state.chat_history.append({"role": "assistant", "content": result})
                else:
                    st.error("è¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç½‘ç»œæ˜¯å¦æ­£å¸¸ã€‚")
